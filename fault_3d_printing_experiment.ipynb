{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMLT0LDo_-QB"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "#from keras.applications import resnet\n",
        "from tensorflow.keras.applications import VGG16,InceptionResNetV2,InceptionV3,ResNet50,ResNet152V2,MobileNetV2,ResNet101,Xception,VGG19,MobileNet,DenseNet201,NASNetMobile,DenseNet169,MobileNet\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from imutils import paths\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "import cv2\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDbtCFmvAWOC"
      },
      "outputs": [],
      "source": [
        "INIT_LR = 1e-3\n",
        "EPOCHS = 50\n",
        "BS = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHSTngIIAY7e",
        "outputId": "4961c60a-8558-4835-89e2-ff931d765daf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] loading images...\n"
          ]
        }
      ],
      "source": [
        "print(\"[INFO] loading images...\")\n",
        "imagePaths =list(paths.list_images(\"/content/drive/MyDrive/3d printing/Experiment2\"))\n",
        "data = []\n",
        "labels = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwXNztj3BCo6"
      },
      "outputs": [],
      "source": [
        "for imagePath in imagePaths:\n",
        "\t# extract the class label from the filename\n",
        "\tlabel = imagePath.split(os.path.sep)[-2]\n",
        "\t# load the image, swap color channels, and resize it to be a fixed\n",
        "\t# 224x224 pixels while ignoring aspect ratio\n",
        "\timage = cv2.imread(imagePath)\n",
        "\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\timage = cv2.resize(image, (224, 224))\n",
        "\t# update the data and labels lists, respectively\n",
        "\tdata.append(image)\n",
        "\tlabels.append(label)\n",
        "# convert the data and labels to NumPy arrays while scaling the pixel\n",
        "# intensities to the range [0, 1]\n",
        "data = np.array(data) / 255.0\n",
        "labels = np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrjOtKhiBJHZ",
        "outputId": "1a96e3a2-9b07-4fa0-e891-305c38d94fe4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "y has 0 samples: array([], dtype=float64)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-f3e8388e3ecc>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# partition the data into training and testing splits using 80% of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# the data for training and the remaining 20% for testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mof\u001b[0m \u001b[0mCSR\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \"\"\"\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    306\u001b[0m             )\n\u001b[1;32m    307\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y has 0 samples: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_input_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: y has 0 samples: array([], dtype=float64)"
          ]
        }
      ],
      "source": [
        "lb = LabelBinarizer()\n",
        "labels = lb.fit_transform(labels)\n",
        "labels = to_categorical(labels)\n",
        "# partition the data into training and testing splits using 80% of\n",
        "# the data for training and the remaining 20% for testing\n",
        "(trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
        "\ttest_size=0.20, random_state=42)\n",
        "# initialize the training data augmentation object\n",
        "trainAug = ImageDataGenerator(\n",
        "\trotation_range=15,\n",
        "\tfill_mode=\"nearest\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSvAZuoMBfKY"
      },
      "outputs": [],
      "source": [
        "# load the VGG16 network, ensuring the head FC layer sets are left\n",
        "# off\n",
        "\n",
        "baseModel =VGG16(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
        "#baseModel =InceptionResNetV2(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
        "#baseModel =ResNet50(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
        "#baseModel =ResNet101(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
        "#baseModel =MobileNetV2(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
        "#baseModel = VGG19(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
        "headModel = baseModel.output\n",
        "headModel = AveragePooling2D(pool_size=(4, 4))(headModel)\n",
        "headModel = Flatten(name=\"flatten\")(headModel)\n",
        "headModel = Dense(64, activation=\"relu\")(headModel)\n",
        "headModel = Dropout(0.5)(headModel)\n",
        "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
        "model = Model(inputs=baseModel.input, outputs=headModel)\n",
        "for layer in baseModel.layers:\n",
        "\tlayer.trainable = False\n",
        "\t# compile our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1huyxWsfBkj7"
      },
      "outputs": [],
      "source": [
        "# compile our model\n",
        "print(\"[INFO] compiling model...\")\n",
        "#opt = Adam(learning_rate=INIT_LR, decay=INIT_LR / EPOCHS)\n",
        "opt = Adam()\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])\n",
        "#model.compile(loss=\"hinge\", optimizer=opt,\n",
        "\t#metrics=[\"accuracy\"])\n",
        "# train the head of the network\n",
        "print(\"[INFO] training head...\")\n",
        "H = model.fit(\n",
        "\ttrainAug.flow(trainX, trainY, batch_size=BS),\n",
        "\tsteps_per_epoch=len(trainX) // BS,\n",
        "\tvalidation_data=(testX, testY),\n",
        "\tvalidation_steps=len(testX) // BS,\n",
        "\tepochs=EPOCHS)\n",
        "#model.save(\"vgg16xray.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7mPtJWCGz4E"
      },
      "outputs": [],
      "source": [
        "pip install quiver_engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1vxfI5cCOFd"
      },
      "outputs": [],
      "source": [
        "#classification report on training\n",
        "predIdxs = model.predict(trainX)\n",
        "# for each image in the testing set we need to find the index of the\n",
        "# label with corresponding largest predicted probability\n",
        "trainpredict = np.argmax(predIdxs, axis=1)\n",
        "# show a nicely formatted classification report\n",
        "print(classification_report(trainY.argmax(axis=1), trainpredict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3a6wuW8ESpB"
      },
      "outputs": [],
      "source": [
        "# compute the confusion matrix and and use it to derive the raw\n",
        "# accuracy, sensitivity, and specificity\n",
        "cm = confusion_matrix(trainY.argmax(axis=1), trainpredict)\n",
        "total = sum(sum(cm))\n",
        "print(cm)\n",
        "acc = (cm[0, 0] + cm[1, 1]) / total\n",
        "sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
        "specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
        "# show the confusion matrix, accuracy, sensitivity, and specificity\n",
        "print(\"acc: {:.4f}\".format(acc))\n",
        "print(\"sensitivity: {:.4f}\".format(sensitivity))\n",
        "print(\"specificity: {:.4f}\".format(specificity))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wG2BC1MDEV8s"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "_, train_acc =model.evaluate(trainX, trainY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-TABUiEEZXT"
      },
      "outputs": [],
      "source": [
        "_, test_acc = model.evaluate(testX, testY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aywMQOZEacD"
      },
      "outputs": [],
      "source": [
        "# make predictions on the testing set\n",
        "print(\"[INFO] evaluating network...\")\n",
        "predIdys = model.predict(testX, batch_size=BS)\n",
        "# for each image in the testing set we need to find the index of the\n",
        "# label with corresponding largest predicted probability\n",
        "testpredict = np.argmax(predIdys, axis=1)\n",
        "# show a nicely formatted classification report\n",
        "print(classification_report(testY.argmax(axis=1), testpredict,\n",
        "\ttarget_names=lb.classes_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzf-ZxCEEcPv"
      },
      "outputs": [],
      "source": [
        "# compute the confusion matrix and and use it to derive the raw\n",
        "# accuracy, sensitivity, and specificity\n",
        "cm = confusion_matrix(testY.argmax(axis=1), testpredict)\n",
        "total = sum(sum(cm))\n",
        "print(cm)\n",
        "acc = (cm[0, 0] + cm[1, 1]) / total\n",
        "sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
        "specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
        "# show the confusion matrix, accuracy, sensitivity, and specificity\n",
        "print(\"acc: {:.4f}\".format(acc))\n",
        "print(\"sensitivity: {:.4f}\".format(sensitivity))\n",
        "print(\"specificity: {:.4f}\".format(specificity))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdP8pPWVEemh"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_auc_score, auc\n",
        "from sklearn.metrics import roc_curve\n",
        "fig = plt.figure(figsize = (4, 3))\n",
        "fpr1,tpr1,_=roc_curve(np.argmax(trainY, axis=1),np.argmax(predIdxs, axis=1))\n",
        "fpr2,tpr2,_=roc_curve(np.argmax(testY, axis=1),np.argmax(predIdys, axis=1))\n",
        "area_under_curve1=auc(fpr1,tpr1)\n",
        "random_probs=[0 for i in range(len(trainY.ravel()))]\n",
        "p_fpr1,p_tpr1,threshold=roc_curve(trainY.ravel(),random_probs, pos_label=1)\n",
        "plt.plot(fpr1,tpr1, label='Train AUC = {:.3f}'.format(area_under_curve1))\n",
        "plt.plot(p_fpr1, p_tpr1)\n",
        "area_under_curve2=auc(fpr2,tpr2)\n",
        "random_probs2=[0 for i in range(len(testY.ravel()))]\n",
        "p_fpr2,p_tpr2,threshold=roc_curve(testY.ravel(),random_probs2, pos_label=1)\n",
        "plt.plot(fpr2,tpr2, label='Test AUC = {:.3f}'.format(area_under_curve2))\n",
        "plt.plot(p_fpr2, p_tpr2)\n",
        "#fpr2,tpr2,_=roc_curve(y_test2.ravel(), yprr2[:,0], pos_label=1)\n",
        "#fpr3,tpr3,_=roc_curve(y_test3.ravel(), yprr3[:,0], pos_label=1)\n",
        "#fpr4,tpr4,_=roc_curve(y_test4.ravel(), yprr4[:,0], pos_label=1)\n",
        "#random_probs2=[0 for i in range(len(y_test2.ravel()))]\n",
        "#random_probs3=[0 for i in range(len(y_test3.ravel()))]\n",
        "#random_probs4=[0 for i in range(len(y_test4.ravel()))]\n",
        "#p_fpr2,p_tpr2,_=roc_curve(y_test2.ravel(),random_probs2, pos_label=1)\n",
        "#p_fpr3,p_tpr3,_=roc_curve(y_test3.ravel(),random_probs3, pos_label=1)\n",
        "#p_fpr4,p_tpr4,_=roc_curve(y_test4.ravel(),random_probs4, pos_label=1)\n",
        "#area_under_curve2 = auc(fpr2, tpr2)\n",
        "#area_under_curve3 = auc(fpr3, tpr3)\n",
        "#area_under_curve4 = auc(fpr4, tpr4)\n",
        "#plt.plot(fpr2,tpr2, label='Borderline-SMOTE= {:.3f}'.format(area_under_curve2))\n",
        "#plt.plot(p_fpr2, p_tpr2, linestyle='--', color='blue')\n",
        "#plt.plot(fpr3,tpr3, label='GAN= {:.3f}'.format(area_under_curve3))\n",
        "#plt.plot(p_fpr3, p_tpr3)\n",
        "#plt.plot(fpr4,tpr4, label='BSGAN= {:.3f}'.format(area_under_curve4))\n",
        "#plt.plot(p_fpr4, p_tpr4)\n",
        "\n",
        "    # x label\n",
        "plt.xlabel('FPR',fontsize=14, fontdict=dict(weight='bold'))\n",
        "    # y label\n",
        "plt.ylabel('TPR', fontsize=14, fontdict=dict(weight='bold'))\n",
        "plt.xticks( rotation=0, weight = 'bold', )\n",
        "plt.yticks( rotation=0, weight = 'bold')\n",
        "plt.tick_params(rotation=0,axis='y', labelsize=12)\n",
        "plt.tick_params(rotation=0,axis='x', labelsize=12)\n",
        "plt.legend()\n",
        "plt.legend(prop={'size':12})\n",
        "#plt.legend(['without oversampling = {:.3f}'.format(area_under_curve1)])\n",
        "#plt.legend(['BorderlineSMOTE = {:.3f}'.format(area_under_curve2)],loc='best')\n",
        "plt.savefig('ROC',dpi=300)\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpVe5tM2EiBp"
      },
      "outputs": [],
      "source": [
        "#import seaborn as sns\n",
        "#plt.style.use('default')\n",
        "from matplotlib.ticker import FormatStrFormatter\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "#fig = plt.figure(figsize = (4, 3))\n",
        "ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import FormatStrFormatter\n",
        "\n",
        "N=50\n",
        "fig, ax = plt.subplots()\n",
        "#plt.rcParams[\"font.family\"] = \"serif\"\n",
        "acc = H.history['accuracy']\n",
        "val_acc = H.history['val_accuracy']\n",
        "#font={'size':10}\n",
        "#matplotlib.rc('font',**font)\n",
        "loss = H.history['loss']\n",
        "val_loss = H.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"TA\")\n",
        "#plt.plot(epochs, loss, label='Training loss')\n",
        "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"VA\")\n",
        "plt.plot(epochs, loss, label='TL')\n",
        "#plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"validation accuracy\")\n",
        "plt.plot(np.arange(0, N), val_loss, label='VS')\n",
        "#plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "#plt.plot(1, val_acc, 'b', label='Validation acc')\n",
        "#plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs',fontsize=16, fontdict=dict(weight='bold'))\n",
        "    # y label\n",
        "plt.ylabel('Accuracy/Loss', fontsize=16, fontdict=dict(weight='bold'))\n",
        "#plt.ylabel(\"Accuracy\")\n",
        "#plt.xlabel(\"Epochs\")\n",
        "plt.xticks( rotation=0, weight = 'bold' )\n",
        "plt.yticks( rotation=0, weight = 'bold')\n",
        "plt.tick_params(rotation=0,axis='y', labelsize=14)\n",
        "plt.tick_params(rotation=0,axis='x', labelsize=14)\n",
        "#plt.grid('white')\n",
        "#plt.grid(axis='x', color='0.95')\n",
        "plt.legend(loc='best')\n",
        "plt.legend(prop={'size':12})\n",
        "fig = plt.figure(figsize = (4, 3))\n",
        "plt.savefig('vgg19acc.png',dpi=1000)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCNF3htUEqY3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0y0S-iTqEt_K"
      },
      "outputs": [],
      "source": [
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSpytJlsEvvC"
      },
      "outputs": [],
      "source": [
        "df=cv2.imread(\"/content/drive/MyDrive/3d printing/Experiment2/badcylinder/IMG_9078.JPG\")\n",
        "df=cv2.resize(df,(224,224))\n",
        "cv2_imshow(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ol-4GRShE5WY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from skimage import data, io, filters,transform\n",
        "import tensorflow\n",
        "\n",
        "import skimage\n",
        "#xi = cv2.imread(\"/content/covid_31.jpg\")\n",
        "#xi = cv2.imread(\"/content/2019-novel-Coronavirus-severe-adult-respiratory-dist_2020_International-Jour-p3-89%0.png\")\n",
        "\n",
        "Xi = skimage.io.imread(\"/content/drive/MyDrive/3d printing/Experiment 2/bad/badcylinder/IMG_9087.JPG\")\n",
        "#Xi = skimage.io.imread(\"/content/drive/MyDrive/monkeypox/augmentedgray/Monekypox_gray_augmented/monkeygray_aug542.jpg\")\n",
        "\n",
        "\n",
        "#xi=cv2.resize(xi,(299,299))\n",
        "#cv2_imshow(xi/2+0.5)\n",
        "\n",
        "Xi = skimage.transform.resize(Xi, (224, 224,3))\n",
        "\n",
        "Xi = (Xi - 0.5)*2 #Inception pre-processing\n",
        "\n",
        "skimage.io.imshow(Xi/2+0.5) # Show image before inception preprocessing\n",
        "\n",
        "\n",
        "\n",
        "#Predict class for image using InceptionV3\n",
        "\n",
        "import keras\n",
        "\n",
        "from keras.applications.imagenet_utils import decode_predictions\n",
        "\n",
        "np.random.seed(222)\n",
        "\n",
        "#inceptionV3_model = tensorflow.keras.applications.inception_v3.InceptionV3() #Load pretrained model\n",
        "#InceptionResNetV2_model=tensorflow.keras.applications.inception_resnet_v2.InceptionResNetV2()\n",
        "#ResNet50_model=tensorflow.keras.applications.resnet50.ResNet50()\n",
        "#ResNet101_model=tensorflow.keras.applications.ResNet101()\n",
        "#Mobilenetv2_model=tensorflow.keras.applications.mobilenet_v2.MobileNetV2()\n",
        "#VGG19_model=tensorflow.keras.applications.vgg19.VGG19()\n",
        "VGG16_model=tensorflow.keras.applications.vgg16.VGG16()\n",
        "\n",
        "#preds = ResNet101_model.predict(Xi[np.newaxis,:,:,:])\n",
        "#preds = ResNet50_model.predict(xi[np.newaxis,:,:,:])\n",
        "#preds = InceptionResNetV2_model.predict(Xi[np.newaxis,:,:,:])\n",
        "#preds = Mobilenetv2_model.predict(Xi[np.newaxis,:,:,:])\n",
        "#preds = VGG19_model.predict(Xi[np.newaxis,:,:,:])\n",
        "preds = VGG16_model.predict(Xi[np.newaxis,:,:,:])\n",
        "\n",
        "\n",
        "top_pred_classes = preds[0].argsort()[-5:][::-1] # Save ids of top 5 classes\n",
        "\n",
        "decode_predictions(preds)[0] #Print top 5 classes\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0rfXbEPFKjZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "import skimage\n",
        "#xi = cv2.imread(\"/content/drive/MyDrive/monkeypox/gray/Monkeypox_gray/monkeypox00.jpg\")\n",
        "#xi = cv2.imread(\"/content/2019-novel-Coronavirus-severe-adult-respiratory-dist_2020_International-Jour-p3-89%0.png\")\n",
        "\n",
        "Xi = skimage.io.imread(\"/content/drive/MyDrive/3d printing/Experiment 2/bad/badcylinder/IMG_9087.JPG\")\n",
        "#Xi = skimage.io.imread(\"/content/drive/MyDrive/monkeypox/augmentedgray/Normal_image_gray_augmented/normalgray_aug105.jpg\")\n",
        "\n",
        "#xi=cv2.resize(xi,(299,299))\n",
        "#cv2_imshow(xi/2+0.5)\n",
        "\n",
        "Xi = skimage.transform.resize(Xi, (299,299))\n",
        "\n",
        "Xi = (Xi - 0.5)*2 #Inception pre-processing\n",
        "\n",
        "skimage.io.imshow(Xi/2+0.5) # Show image before inception preprocessing\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ps6fqayjFTkL"
      },
      "outputs": [],
      "source": [
        "#Generate segmentation for image\n",
        "\n",
        "import skimage.segmentation\n",
        "Xi = skimage.transform.resize(Xi, (299,299,3))\n",
        "superpixels = skimage.segmentation.quickshift(Xi, kernel_size=4,max_dist=200, ratio=0.2)\n",
        "\n",
        "num_superpixels = np.unique(superpixels).shape[0]\n",
        "\n",
        "skimage.io.imshow(skimage.segmentation.mark_boundaries(Xi/2+0.5, superpixels))\n",
        "\n",
        "\n",
        "\n",
        "#Generate perturbations\n",
        "\n",
        "num_perturb = 150\n",
        "\n",
        "perturbations = np.random.binomial(1, 0.5, size=(num_perturb, num_superpixels))\n",
        "\n",
        "\n",
        "\n",
        "#Create function to apply perturbations to images\n",
        "\n",
        "import copy\n",
        "\n",
        "def perturb_image(img,perturbation,segments):\n",
        "\n",
        "  active_pixels = np.where(perturbation == 1)[0]\n",
        "\n",
        "  mask = np.zeros(segments.shape)\n",
        "\n",
        "  for active in active_pixels:\n",
        "\n",
        "      mask[segments == active] = 1\n",
        "\n",
        "  perturbed_image = copy.deepcopy(img)\n",
        "\n",
        "  perturbed_image = perturbed_image*mask[:,:,np.newaxis]\n",
        "\n",
        "  return perturbed_image\n",
        "\n",
        "\n",
        "\n",
        "#Show example of perturbations\n",
        "\n",
        "print(perturbations[0])\n",
        "\n",
        "skimage.io.imshow(perturb_image(Xi/2+0.5,perturbations[0],superpixels))\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HATN2Ge3FXPm"
      },
      "outputs": [],
      "source": [
        "from skimage import io\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "#url = '/content/drive/MyDrive/monkeypox/gray/Normal_image_gray/normal00.jpg'\n",
        "url=\"/content/drive/MyDrive/3d printing/Experiment 2/bad/badcylinder/IMG_9087.JPG\"\n",
        "#url=\"/content/drive/MyDrive/monkeypox/augmentedgray/Monekypox_gray_augmented/monkeygray_aug542.jpg\"\n",
        "#url=\"/content/drive/MyDrive/monkeypox/augmentedgray/Normal_image_gray_augmented/normalgray_aug105.jpg\"\n",
        "\n",
        "def read_and_transform_img(url):\n",
        "\n",
        "    img = skimage.io.imread(url)\n",
        "    img = skimage.transform.resize(img, (224, 224,3))\n",
        "\n",
        "    img = image.img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "\n",
        "    return img\n",
        "\n",
        "images = read_and_transform_img(url)\n",
        "\n",
        "preds = model.predict(images)\n",
        "prediction = np.argmax(preds)\n",
        "pct = np.max(preds)\n",
        "\n",
        "if prediction == 0:\n",
        "    print('It\\'s a good!')\n",
        "elif prediction == 1:\n",
        "    print('It\\'s a bad!')\n",
        "else:\n",
        "    print('Unable to predict!')\n",
        "\n",
        "print(pct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wh9WE_aKGP98"
      },
      "outputs": [],
      "source": [
        "pip install lime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0BaIMRdGS9m"
      },
      "outputs": [],
      "source": [
        "from lime import lime_image\n",
        "\n",
        "explainer = lime_image.LimeImageExplainer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSy-TNqXGWDe"
      },
      "outputs": [],
      "source": [
        "explanation = explainer.explain_instance(images[0].astype('double'), model.predict,\n",
        "                                         top_labels=3, hide_color=0, num_samples=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtDvKyxLGa3N"
      },
      "outputs": [],
      "source": [
        "from skimage.segmentation import mark_boundaries\n",
        "\n",
        "temp_1, mask_1 = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=True)\n",
        "temp_2, mask_2 = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=False, num_features=10, hide_rest=False)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(5,5))\n",
        "ax1.imshow(mark_boundaries(temp_1, mask_1))\n",
        "ax2.imshow(mark_boundaries(temp_2, mask_2))\n",
        "ax1.axis('off')\n",
        "ax2.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDXEgIr4Geax"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "res_model = ResNet50()\n",
        "res_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8r28-s8GiSx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import zoom\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "#img = cv2.imread('/content/drive/MyDrive/3d printing/Experiment 2/goodcylinder/IMG_8935.JPG')\n",
        "img = cv2.imread('/content/drive/MyDrive/3d printing/Experiment 2/bad/badcylinder/IMG_9087.JPG')\n",
        "#img = cv2.imread(\"/content/drive/MyDrive/monkeypox/augmentedgray/Monekypox_gray_augmented/monkeygray_aug542.jpg\")\n",
        "img = cv2.resize(img, (224, 224))\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "X = np.expand_dims(img, axis=0).astype(np.float32)\n",
        "X = preprocess_input(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRHP93XfGrKL"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "conv_output = res_model.get_layer(\"conv5_block3_out\").output\n",
        "pred_ouptut = res_model.get_layer(\"predictions\").output\n",
        "model = Model(res_model.input, outputs=[conv_output, pred_ouptut])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqBwxdCaGtig"
      },
      "outputs": [],
      "source": [
        "conv, pred = model.predict(X)\n",
        "decode_predictions(pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-NQq62mGwEM"
      },
      "outputs": [],
      "source": [
        "scale = 224 / 7\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(9):\n",
        "    plt.xticks( rotation=0, weight = 'bold' )\n",
        "    plt.yticks( rotation=0, weight = 'bold')\n",
        "    plt.tick_params(rotation=0,axis='y', labelsize=12)\n",
        "    plt.tick_params(rotation=0,axis='x', labelsize=12)\n",
        "    plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(img)\n",
        "    #plt.imshow(zoom(conv[0, :,:,i], zoom=(scale, scale)))\n",
        "    plt.imshow(zoom(conv[0, :,:,i], zoom=(scale, scale)), cmap='jet', alpha=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slwJWO2pGypY"
      },
      "outputs": [],
      "source": [
        "target = np.argmax(pred, axis=1).squeeze()\n",
        "w, b = model.get_layer(\"predictions\").weights\n",
        "weights = w[:, target].numpy()\n",
        "heatmap = conv.squeeze() @ weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFqDKEjmG4Bw"
      },
      "outputs": [],
      "source": [
        "scale = 224 / 7\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.xticks( rotation=0, weight = 'bold' )\n",
        "plt.yticks( rotation=0, weight = 'bold')\n",
        "plt.tick_params(rotation=0,axis='y', labelsize=14)\n",
        "plt.tick_params(rotation=0,axis='x', labelsize=14)\n",
        "plt.imshow(img)\n",
        "plt.imshow(zoom(heatmap, zoom=(scale, scale)), cmap='jet', alpha=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dVvC8OgG5q-"
      },
      "outputs": [],
      "source": [
        "!pip install gradcam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZNiWhdeG713"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "\n",
        "layer_finders = {}\n",
        "\n",
        "\n",
        "def register_layer_finder(model_type):\n",
        "    def register(func):\n",
        "        layer_finders[model_type] = func\n",
        "        return func\n",
        "    return register\n",
        "\n",
        "\n",
        "def visualize_cam(mask, img, alpha=1.0):\n",
        "    \"\"\"Make heatmap from mask and synthesize GradCAM result image using heatmap and img.\n",
        "    Args:\n",
        "        mask (torch.tensor): mask shape of (1, 1, H, W) and each element has value in range [0, 1]\n",
        "        img (torch.tensor): img shape of (1, 3, H, W) and each pixel value is in range [0, 1]\n",
        "    Return:\n",
        "        heatmap (torch.tensor): heatmap img shape of (3, H, W)\n",
        "        result (torch.tensor): synthesized GradCAM result of same shape with heatmap.\n",
        "    \"\"\"\n",
        "    heatmap = (255 * mask.squeeze()).type(torch.uint8).cpu().numpy()\n",
        "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "    heatmap = torch.from_numpy(heatmap).permute(2, 0, 1).float().div(255)\n",
        "    b, g, r = heatmap.split(1)\n",
        "    heatmap = torch.cat([r, g, b]) * alpha\n",
        "\n",
        "    result = heatmap+img.cpu()\n",
        "    result = result.div(result.max()).squeeze()\n",
        "\n",
        "    return heatmap, result\n",
        "\n",
        "\n",
        "@register_layer_finder('resnet')\n",
        "def find_resnet_layer(arch, target_layer_name):\n",
        "    \"\"\"Find resnet layer to calculate GradCAM and GradCAM++\n",
        "    Args:\n",
        "        arch: default torchvision densenet models\n",
        "        target_layer_name (str): the name of layer with its hierarchical information. please refer to usages below.\n",
        "            target_layer_name = 'conv1'\n",
        "            target_layer_name = 'layer1'\n",
        "            target_layer_name = 'layer1_basicblock0'\n",
        "            target_layer_name = 'layer1_basicblock0_relu'\n",
        "            target_layer_name = 'layer1_bottleneck0'\n",
        "            target_layer_name = 'layer1_bottleneck0_conv1'\n",
        "            target_layer_name = 'layer1_bottleneck0_downsample'\n",
        "            target_layer_name = 'layer1_bottleneck0_downsample_0'\n",
        "            target_layer_name = 'avgpool'\n",
        "            target_layer_name = 'fc'\n",
        "    Return:\n",
        "        target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "    if 'layer' in target_layer_name:\n",
        "        hierarchy = target_layer_name.split('_')\n",
        "        layer_num = int(hierarchy[0].lstrip('layer'))\n",
        "        if layer_num == 1:\n",
        "            target_layer = arch.layer1\n",
        "        elif layer_num == 2:\n",
        "            target_layer = arch.layer2\n",
        "        elif layer_num == 3:\n",
        "            target_layer = arch.layer3\n",
        "        elif layer_num == 4:\n",
        "            target_layer = arch.layer4\n",
        "        else:\n",
        "            raise ValueError('unknown layer : {}'.format(target_layer_name))\n",
        "\n",
        "        if len(hierarchy) >= 2:\n",
        "            bottleneck_num = int(hierarchy[1].lower().lstrip('bottleneck').lstrip('basicblock'))\n",
        "            target_layer = target_layer[bottleneck_num]\n",
        "\n",
        "        if len(hierarchy) >= 3:\n",
        "            target_layer = target_layer._modules[hierarchy[2]]\n",
        "\n",
        "        if len(hierarchy) == 4:\n",
        "            target_layer = target_layer._modules[hierarchy[3]]\n",
        "\n",
        "    else:\n",
        "        target_layer = arch._modules[target_layer_name]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "@register_layer_finder('densenet')\n",
        "def find_densenet_layer(arch, target_layer_name):\n",
        "    \"\"\"Find densenet layer to calculate GradCAM and GradCAM++\n",
        "    Args:\n",
        "        arch: default torchvision densenet models\n",
        "        target_layer_name (str): the name of layer with its hierarchical information. please refer to usages below.\n",
        "            target_layer_name = 'features'\n",
        "            target_layer_name = 'features_transition1'\n",
        "            target_layer_name = 'features_transition1_norm'\n",
        "            target_layer_name = 'features_denseblock2_denselayer12'\n",
        "            target_layer_name = 'features_denseblock2_denselayer12_norm1'\n",
        "            target_layer_name = 'features_denseblock2_denselayer12_norm1'\n",
        "            target_layer_name = 'classifier'\n",
        "    Return:\n",
        "        target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "\n",
        "    hierarchy = target_layer_name.split('_')\n",
        "    target_layer = arch._modules[hierarchy[0]]\n",
        "\n",
        "    if len(hierarchy) >= 2:\n",
        "        target_layer = target_layer._modules[hierarchy[1]]\n",
        "\n",
        "    if len(hierarchy) >= 3:\n",
        "        target_layer = target_layer._modules[hierarchy[2]]\n",
        "\n",
        "    if len(hierarchy) == 4:\n",
        "        target_layer = target_layer._modules[hierarchy[3]]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "@register_layer_finder('vgg')\n",
        "def find_vgg_layer(arch, target_layer_name):\n",
        "    \"\"\"Find vgg layer to calculate GradCAM and GradCAM++\n",
        "    Args:\n",
        "        arch: default torchvision densenet models\n",
        "        target_layer_name (str): the name of layer with its hierarchical information. please refer to usages below.\n",
        "            target_layer_name = 'features'\n",
        "            target_layer_name = 'features_42'\n",
        "            target_layer_name = 'classifier'\n",
        "            target_layer_name = 'classifier_0'\n",
        "    Return:\n",
        "        target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "    hierarchy = target_layer_name.split('_')\n",
        "\n",
        "    if len(hierarchy) >= 1:\n",
        "        target_layer = arch.features\n",
        "\n",
        "    if len(hierarchy) == 2:\n",
        "        target_layer = target_layer[int(hierarchy[1])]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "@register_layer_finder('alexnet')\n",
        "def find_alexnet_layer(arch, target_layer_name):\n",
        "    \"\"\"Find alexnet layer to calculate GradCAM and GradCAM++\n",
        "    Args:\n",
        "        arch: default torchvision densenet models\n",
        "        target_layer_name (str): the name of layer with its hierarchical information. please refer to usages below.\n",
        "            target_layer_name = 'features'\n",
        "            target_layer_name = 'features_0'\n",
        "            target_layer_name = 'classifier'\n",
        "            target_layer_name = 'classifier_0'\n",
        "    Return:\n",
        "        target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "    hierarchy = target_layer_name.split('_')\n",
        "\n",
        "    if len(hierarchy) >= 1:\n",
        "        target_layer = arch.features\n",
        "\n",
        "    if len(hierarchy) == 2:\n",
        "        target_layer = target_layer[int(hierarchy[1])]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "@register_layer_finder('squeezenet')\n",
        "def find_squeezenet_layer(arch, target_layer_name):\n",
        "    \"\"\"Find squeezenet layer to calculate GradCAM and GradCAM++\n",
        "    Args:\n",
        "        arch: default torchvision densenet models\n",
        "        target_layer_name (str): the name of layer with its hierarchical information. please refer to usages below.\n",
        "            target_layer_name = 'features_12'\n",
        "            target_layer_name = 'features_12_expand3x3'\n",
        "            target_layer_name = 'features_12_expand3x3_activation'\n",
        "    Return:\n",
        "        target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "    hierarchy = target_layer_name.split('_')\n",
        "    target_layer = arch._modules[hierarchy[0]]\n",
        "\n",
        "    if len(hierarchy) >= 2:\n",
        "        target_layer = target_layer._modules[hierarchy[1]]\n",
        "\n",
        "    if len(hierarchy) == 3:\n",
        "        target_layer = target_layer._modules[hierarchy[2]]\n",
        "\n",
        "    elif len(hierarchy) == 4:\n",
        "        target_layer = target_layer._modules[hierarchy[2]+'_'+hierarchy[3]]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "def denormalize(tensor, mean, std):\n",
        "    if not tensor.ndimension() == 4:\n",
        "        raise TypeError('tensor should be 4D')\n",
        "\n",
        "    mean = torch.FloatTensor(mean).view(1, 3, 1, 1).expand_as(tensor).to(tensor.device)\n",
        "    std = torch.FloatTensor(std).view(1, 3, 1, 1).expand_as(tensor).to(tensor.device)\n",
        "\n",
        "    return tensor.mul(std).add(mean)\n",
        "\n",
        "\n",
        "def normalize(tensor, mean, std):\n",
        "    if not tensor.ndimension() == 4:\n",
        "        raise TypeError('tensor should be 4D')\n",
        "\n",
        "    mean = torch.FloatTensor(mean).view(1, 3, 1, 1).expand_as(tensor).to(tensor.device)\n",
        "    std = torch.FloatTensor(std).view(1, 3, 1, 1).expand_as(tensor).to(tensor.device)\n",
        "\n",
        "    return tensor.sub(mean).div(std)\n",
        "\n",
        "\n",
        "class Normalize(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        return self.do(tensor)\n",
        "\n",
        "    def do(self, tensor):\n",
        "        return normalize(tensor, self.mean, self.std)\n",
        "\n",
        "    def undo(self, tensor):\n",
        "        return denormalize(tensor, self.mean, self.std)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gS67B_L5HCET"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#from .utils import layer_finders\n",
        "\n",
        "\n",
        "class GradCAM:\n",
        "    \"\"\"Calculate GradCAM salinecy map.\n",
        "    Args:\n",
        "        input: input image with shape of (1, 3, H, W)\n",
        "        class_idx (int): class index for calculating GradCAM.\n",
        "                If not specified, the class index that makes the highest model prediction score will be used.\n",
        "    Return:\n",
        "        mask: saliency map of the same spatial dimension with input\n",
        "        logit: model output\n",
        "    A simple example:\n",
        "        # initialize a model, model_dict and gradcam\n",
        "        resnet = torchvision.models.resnet101(pretrained=True)\n",
        "        resnet.eval()\n",
        "        gradcam = GradCAM.from_config(model_type='resnet', arch=resnet, layer_name='layer4')\n",
        "        # get an image and normalize with mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)\n",
        "        img = load_img()\n",
        "        normed_img = normalizer(img)\n",
        "        # get a GradCAM saliency map on the class index 10.\n",
        "        mask, logit = gradcam(normed_img, class_idx=10)\n",
        "        # make heatmap from mask and synthesize saliency map using heatmap and img\n",
        "        heatmap, cam_result = visualize_cam(mask, img)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, arch: torch.nn.Module, target_layer: torch.nn.Module):\n",
        "        self.model_arch = arch\n",
        "\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, arch: torch.nn.Module, model_type: str, layer_name: str):\n",
        "        target_layer = layer_finders[model_type](arch, layer_name)\n",
        "        return cls(arch, target_layer)\n",
        "\n",
        "    def saliency_map_size(self, *input_size):\n",
        "        device = next(self.model_arch.parameters()).device\n",
        "        self.model_arch(torch.zeros(1, 3, *input_size, device=device))\n",
        "        return self.activations['value'].shape[2:]\n",
        "\n",
        "    def forward(self, input, class_idx=None, retain_graph=False):\n",
        "        b, c, h, w = input.size()\n",
        "\n",
        "        logit = self.model_arch(input)\n",
        "        if class_idx is None:\n",
        "            score = logit[:, logit.max(1)[-1]].squeeze()\n",
        "        else:\n",
        "            score = logit[:, class_idx].squeeze()\n",
        "\n",
        "        self.model_arch.zero_grad()\n",
        "        score.backward(retain_graph=retain_graph)\n",
        "        gradients = self.gradients['value']\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = gradients.size()\n",
        "\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        # alpha = F.relu(gradients.view(b, k, -1)).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "\n",
        "        saliency_map = (weights*activations).sum(1, keepdim=True)\n",
        "        saliency_map = F.relu(saliency_map)\n",
        "        saliency_map = F.upsample(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "        saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "        saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "\n",
        "        return saliency_map, logit\n",
        "\n",
        "    def __call__(self, input, class_idx=None, retain_graph=False):\n",
        "        return self.forward(input, class_idx, retain_graph)\n",
        "\n",
        "\n",
        "class GradCAMpp(GradCAM):\n",
        "    \"\"\"Calculate GradCAM++ salinecy map.\n",
        "    Args:\n",
        "        input: input image with shape of (1, 3, H, W)\n",
        "        class_idx (int): class index for calculating GradCAM.\n",
        "                If not specified, the class index that makes the highest model prediction score will be used.\n",
        "    Return:\n",
        "        mask: saliency map of the same spatial dimension with input\n",
        "        logit: model output\n",
        "    A simple example:\n",
        "        # initialize a model, model_dict and gradcampp\n",
        "        resnet = torchvision.models.resnet101(pretrained=True)\n",
        "        resnet.eval()\n",
        "        gradcampp = GradCAMpp.from_config(model_type='resnet', arch=resnet, layer_name='layer4')\n",
        "        # get an image and normalize with mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)\n",
        "        img = load_img()\n",
        "        normed_img = normalizer(img)\n",
        "        # get a GradCAM saliency map on the class index 10.\n",
        "        mask, logit = gradcampp(normed_img, class_idx=10)\n",
        "        # make heatmap from mask and synthesize saliency map using heatmap and img\n",
        "        heatmap, cam_result = visualize_cam(mask, img)\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, input, class_idx=None, retain_graph=False):\n",
        "        b, c, h, w = input.size()\n",
        "\n",
        "        logit = self.model_arch(input)\n",
        "        if class_idx is None:\n",
        "            score = logit[:, logit.max(1)[-1]].squeeze()\n",
        "        else:\n",
        "            score = logit[:, class_idx].squeeze()\n",
        "\n",
        "        self.model_arch.zero_grad()\n",
        "        score.backward(retain_graph=retain_graph)\n",
        "        gradients = self.gradients['value']  # dS/dA\n",
        "        activations = self.activations['value']  # A\n",
        "        b, k, u, v = gradients.size()\n",
        "\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = alpha_num.mul(2) + activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "\n",
        "        alpha = alpha_num.div(alpha_denom+1e-7)\n",
        "        positive_gradients = F.relu(score.exp()*gradients)  # ReLU(dY/dA) == ReLU(exp(S)*dS/dA))\n",
        "        weights = (alpha*positive_gradients).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "        saliency_map = (weights*activations).sum(1, keepdim=True)\n",
        "        saliency_map = F.relu(saliency_map)\n",
        "        saliency_map = F.upsample(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "        saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "        saliency_map = (saliency_map-saliency_map_min).div(saliency_map_max-saliency_map_min).data\n",
        "\n",
        "        return saliency_map, logit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iCGlJ47HFxv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import PIL\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid, save_image\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slzjPOYBHHAW"
      },
      "outputs": [],
      "source": [
        "img_dir = '/content/drive/MyDrive/3d printing/Experiment 2/bad/badcylinder'\n",
        "#img_dir = '/content/drive/MyDrive/monkeypox/augmentedgray/Monekypox_gray_augmented'\n",
        "img_dir = '/content'\n",
        "# img_name = 'collies.JPG'\n",
        "# img_name = 'multiple_dogs.jpg'\n",
        "# img_name = 'snake.JPEG'\n",
        "img_name = '34.jfif'\n",
        "img_path = os.path.join(img_dir, img_name)\n",
        "\n",
        "pil_img = PIL.Image.open(img_path)\n",
        "pil_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJG-uKosHmP2"
      },
      "outputs": [],
      "source": [
        "torch_img = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])(pil_img).to(device)\n",
        "normed_torch_img = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(torch_img)[None]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmQ46S7FHql9"
      },
      "outputs": [],
      "source": [
        "vgg = models.vgg16(pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8H8AYVxHuCR"
      },
      "outputs": [],
      "source": [
        "configs = [  dict(model_type='vgg', arch=vgg, layer_name='features_29')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lYifUTAHwfz"
      },
      "outputs": [],
      "source": [
        "alexnet = models.alexnet(pretrained=True)\n",
        "vgg = models.vgg16(pretrained=True)\n",
        "resnet = models.resnet101(pretrained=True)\n",
        "densenet = models.densenet161(pretrained=True)\n",
        "squeezenet = models.squeezenet1_1(pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfFRHk3AH2hg"
      },
      "outputs": [],
      "source": [
        "configs = [\n",
        "    #dict(model_type='alexnet', arch=alexnet, layer_name='features_11'),\n",
        "    dict(model_type='vgg', arch=vgg, layer_name='features_29'),\n",
        "    #dict(model_type='resnet', arch=resnet, layer_name='layer4'),\n",
        "    #dict(model_type='densenet', arch=densenet, layer_name='features_norm5'),\n",
        "    #dict(model_type='squeezenet', arch=squeezenet, layer_name='features_12_expand3x3_activation')\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbIFUVcmH4BO"
      },
      "outputs": [],
      "source": [
        "for config in configs:\n",
        "    config['arch'].to(device).eval()\n",
        "\n",
        "cams = [\n",
        "    [cls.from_config(**config) for cls in (GradCAM, GradCAMpp)]\n",
        "    for config in configs\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCYsqD5FH7mJ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(2, 2))\n",
        "#plt.xticks( rotation=0, weight = 'bold' )\n",
        "#plt.yticks( rotation=0, weight = 'bold')\n",
        "#plt.tick_params(rotation=0,axis='y', labelsize=14)\n",
        "#plt.tick_params(rotation=0,axis='x', labelsize=14)\n",
        "#plt.imshow(img)\n",
        "images = []\n",
        "for gradcam, gradcam_pp in cams:\n",
        "    mask, _ = gradcam(normed_torch_img)\n",
        "    heatmap, result = visualize_cam(mask, torch_img)\n",
        "\n",
        "    mask_pp, _ = gradcam_pp(normed_torch_img)\n",
        "    heatmap_pp, result_pp = visualize_cam(mask_pp, torch_img)\n",
        "\n",
        "    images.extend([torch_img.cpu(), heatmap, heatmap_pp, result, result_pp])\n",
        "\n",
        "\n",
        "grid_image = make_grid(images, nrow=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWPhoaSfH_kQ"
      },
      "outputs": [],
      "source": [
        "transforms.ToPILImage()(grid_image)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}